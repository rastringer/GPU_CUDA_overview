{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRQDJ1qCSVQvrgxVrnqu1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/GPU_CUDA_overview/blob/main/c_to_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with CUDA\n",
        "\n",
        "In this notebook, we dive into basic CUDA programming in C. If you don't know C well, don't worry, the code is straightforward with a focus on the CUDA considerations. Doing the exercises and following the examples can help with low-level understanding, which can often be abstracted away by the equivalent Python libraries.\n",
        "\n",
        "## Resources\n",
        "\n",
        "We can do all exercises on the free `T4` GPU on Colab.\n",
        "\n",
        "Let's check we have the Nvidia CUDA Compiler Driver (NVCC) installed:"
      ],
      "metadata": {
        "id": "tnL5NMYITkDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiprhHaDTjX4",
        "outputId": "ccff3939-bdf3-49e3-dd9f-dffaa1649f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLUoUO7cVu_o",
        "outputId": "474ac184-fb03-4896-f8d9-cbb2ec5ff6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znEdqj5BV4Xy",
        "outputId": "fe26075d-e44f-4525-c6ca-e665869a3e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpue_eww0_\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing and running C code in a Colab\n",
        "\n",
        "Thanks to the `nvcc4jupyter` extension, we can run C/C++ code from our notebook cells.\n",
        "\n",
        "Simply annotate each code cell with `%%cuda` at the top.\n",
        "\n",
        "Syntax checking may mean a lot of read and yellow lines on our code, since it's focused on Python code, so it's best to turn this feature off:\n",
        "Settings -> Editor -> Code diagnostics -> None.\n",
        "\n",
        "## Hello, World!"
      ],
      "metadata": {
        "id": "FcykuBJZUbIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "void hello()\n",
        "{\n",
        "  printf(\"Hello from the CPU.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  hello();\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB27oYQ4WAKi",
        "outputId": "6c58ab15-23ab-4cda-e0ec-7c71279ee536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from the CPU.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## and from the GPU\n",
        "\n",
        "Let's adjust the code to make it run on the GPU.\n",
        "\n",
        "We will need to annotate functions with\n",
        "\n",
        "`__global__`\n",
        "\n",
        "and synchronize our code on the completion of the kernel using the method\n",
        "\n",
        "`cudaDeviceSynchronize();`\n"
      ],
      "metadata": {
        "id": "-y-bSoxsWxb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void helloGPU()\n",
        "{\n",
        "  printf(\"Hello from the GPU.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  helloGPU<<<1, 1>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TWryfzTXQLR",
        "outputId": "fc81db06-cd42-4ae6-ccd2-622492dc2898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from the GPU.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blocks and threads\n",
        "\n",
        "You may be wondering why we have triple angle brackets in the function call:\n",
        "\n",
        "`<<<1, 1>>>`\n",
        "\n",
        "These are required parameters for CUDA denoting the blocks and threads in which our tasks should run.\n",
        "\n",
        "`<<< NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>`\n",
        "\n",
        "Threads and blocks are fundamental for organizing parallel computation on GPUs. Threads are the smallest unit of action, each capable of running a single instance of the kernel function.\n",
        "\n",
        "Threads are grouped into blocks, where they can cooperate and share resources.\n",
        "\n",
        "Multiple blocks form a grid, the highest level of CUDA hierarchy.\n",
        "\n",
        "\n",
        "`kernelA <<<1, 1>>>()` runs one block with a single thread so will run only once.\n",
        "\n",
        "`kernelB <<<1, 10>>>()` runs one block with 10 threads and will run 10 times.\n",
        "\n",
        "`kernelC <<<10, 1>>>()` runs 10 thread blocks, each with a single thread so will run 10 times.\n",
        "\n",
        "`kernelD <<<10, 10>>>()` runs 10 blocks which each have 10 thread, so run 100 times.\n",
        "\n",
        "The next example illustrates the use of threads for parallel action. Experiment with changing the `<<<blocks, threads>>>` and see the results."
      ],
      "metadata": {
        "id": "HgEEx4_lYUo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void printInts()\n",
        "{\n",
        "  for(int i=0; i<10; i++)\n",
        "  {\n",
        "      printf(\"%d \", i);\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  printInts<<<1, 1>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}\n"
      ],
      "metadata": {
        "id": "DyNxJRk_gP_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3685b8-3503-471f-b4d4-832477d3a3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1 2 3 4 5 6 7 8 9 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Threads and indexes\n",
        "\n",
        "Each thread has an index denoting its place in a block. Blocks also are indexed, and grouped into a grid.\n",
        "\n",
        "There are useful methods for identifying these indexes:\n",
        "\n",
        "`threadIdx.x` : identifies the index of the thread\n",
        "`blockIdx.x` : identifies the index of the block\n",
        "`blockDim.x` : represents the number of threads in a block\n",
        "\n",
        "For example, here is a classical loop:\n"
      ],
      "metadata": {
        "id": "ubJtCKsreJJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "void loop(int n)\n",
        "{\n",
        "    for(int i=0; i<n; i++)\n",
        "  {\n",
        "      printf(\"This is loop cycle %d \", i);\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  loop(10);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGnrdLJgf5Hi",
        "outputId": "b6efca32-89af-4692-fbc4-f375284de392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is loop cycle 0 This is loop cycle 1 This is loop cycle 2 This is loop cycle 3 This is loop cycle 4 This is loop cycle 5 This is loop cycle 6 This is loop cycle 7 This is loop cycle 8 This is loop cycle 9 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can accelerate this operation by launching the iterations in parallel, a *multi-block loop*. Let's use two blocks of threads.\n",
        "\n",
        "Here, `blockIdx.x * blockDim.x + threadIdx.x` gives threads unique indexes in a grid."
      ],
      "metadata": {
        "id": "EutExroFgLUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void loop()\n",
        "{\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  printf(\"This is loop cycle %d\\n\", i);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "  loop<<<2, 5>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFiX0r_KdrIu",
        "outputId": "c05adf44-d09d-4bd1-854b-1fd6dc19d8d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is loop cycle 5\n",
            "This is loop cycle 6\n",
            "This is loop cycle 7\n",
            "This is loop cycle 8\n",
            "This is loop cycle 9\n",
            "This is loop cycle 0\n",
            "This is loop cycle 1\n",
            "This is loop cycle 2\n",
            "This is loop cycle 3\n",
            "This is loop cycle 4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Allocating memory\n",
        "\n",
        "CUDA version 6 and above has simplified memory allocation for both the CPU host and as or or many GPU devices with little additional work necessary by the developer.\n",
        "\n",
        "C uses calls to `malloc` and `free` to allocate and liberate memory; we simply  replace these with `cudaMallocManaged` and `cudaFree`."
      ],
      "metadata": {
        "id": "aXjfTlK_gtSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// CPU-only\n",
        "\n",
        "int N = 2<<20;\n",
        "size_t size = N * sizeof(int);\n",
        "\n",
        "int *a = (int *)malloc(size);\n",
        "\n",
        "int square(int x)\n",
        "{\n",
        "  return x = x * x;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    a[0] = 5;\n",
        "    int result = square(a[0]);\n",
        "    printf(\"Result is %d\\n\", result);\n",
        "    free(a);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gTky-5biMAf",
        "outputId": "3f9cd3f6-960c-43b7-e386-edd584e2c25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result is 25\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: CUDA version\n",
        "\n",
        "Let's use `cudaMallocManaged` and `cudaFree`. Developers often prefix variables to be put on the device with `d_`, we will write `device_` to make this clear."
      ],
      "metadata": {
        "id": "0Fw8TsktkXNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Accelerated\n",
        "\n",
        "int N = 10;\n",
        "size_t size = N * sizeof(int);\n",
        "\n",
        "int *device_a;\n",
        "\n",
        "/*\n",
        " Here is our earlier square function:\n",
        " int square(int x)\n",
        "{\n",
        "  return x = x * x;\n",
        "}\n",
        " */\n",
        "\n",
        "// Initialize the array\n",
        "void init(int *a, int N)\n",
        "{\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i)\n",
        "  {\n",
        "    a[i] = i;\n",
        "  }\n",
        "}\n",
        "\n",
        "/*\n",
        " Here is our earlier square function:\n",
        " int square(int x)\n",
        "{\n",
        "  return x = x * x;\n",
        "}\n",
        "How can we make this into a CUDA function?\n",
        "Remember the threadIdx.x etc methods to\n",
        "create a unique index for each thread across all blocks\n",
        " */\n",
        "\n",
        "__global__ void square_kernel(int *device_a, int n)\n",
        "{\n",
        "    // initialize an index variable\n",
        "    // Your code here;\n",
        "    if (idx < n) {\n",
        "        // square the device array at the index\n",
        "        // Your code here\n",
        "    }\n",
        "}\n",
        "\n",
        "// Use `a` on the CPU and/or on any GPU in the accelerated system.\n",
        "\n",
        "int main() {\n",
        "    // Use cudaMallocManaged to allocate memory for the device array\n",
        "    // and the size variable\n",
        "    // Your code here\n",
        "\n",
        "    // Initialize the array\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        device_a[i] = i + 1;  // Values will be 1, 2, 3, ..., 10\n",
        "    }\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    square_kernel<<<blocksPerGrid, threadsPerBlock>>>(device_a, N);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    printf(\"Squared array:\\n\");\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        printf(\"%d \", device_a[i]);\n",
        "    }\n",
        "\n",
        "    cudaFree(device_a);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "QW4mt_aZEYzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ],
      "metadata": {
        "id": "tGGHYD3KHjVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Accelerated\n",
        "\n",
        "int N = 312;\n",
        "size_t size = N * sizeof(int);\n",
        "\n",
        "int *device_a;\n",
        "\n",
        "__global__ void square_kernel(int *device_a, int n)\n",
        "{\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < n) {\n",
        "        device_a[idx] = device_a[idx] * device_a[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Use `a` on the CPU and/or on any GPU in the accelerated system.\n",
        "\n",
        "int main() {\n",
        "    // Note the address of `a` is passed as first argument.\n",
        "    cudaMallocManaged(&device_a, size);\n",
        "\n",
        "    // Initialize the array\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        device_a[i] = i + 1;  // Values will be 1, 2, 3, ..., 10\n",
        "    }\n",
        "\n",
        "    int threadsPerBlock = 8;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    square_kernel<<<blocksPerGrid, threadsPerBlock>>>(device_a, N);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    printf(\"Squared array:\\n\");\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        printf(\"%d \", device_a[i]);\n",
        "    }\n",
        "\n",
        "    cudaFree(device_a);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT7agwx-i4ep",
        "outputId": "efb7a972-4597-44d9-ca71-2cc28c91b610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid stride\n",
        "\n",
        "Let's remind ourselves: grids are the highest level of the GPU hierarchy:\n",
        "\n",
        "Threads -> Blocks -> Grids\n",
        "\n",
        "Often in CUDA programming, the number of threads in a grid is smaller than the data upon which we operate.\n",
        "\n",
        "If we have an array of 100 elements to do something with, and a grid of 25 threads, each grid will have to perform computation 4 times.\n",
        "\n",
        "A thread at index 20 in the grid would:\n",
        "\n",
        "* Perform its operation (eg squaring the element) on element 20 of the array\n",
        "* Increment its index by 25, the size of the grid, to 45\n",
        "* Perform its operation on element 45 of the array\n",
        "* Increment its index by 25, to 70\n",
        "* Perform its operation on element 70 of the array\n",
        "* Increment its index by 25, to 95\n",
        "* Perform its operation on element 95 of the array\n",
        "* Stop its work, since 120 is out of range for the array\n",
        "\n",
        "We can use the CUDA variable `gridDim.x` to calculate the number of blocks in the grid. Then we calculate the number of threads in each block, using\n",
        "\n",
        "```\n",
        "gridDim.x * blockDim.x\n",
        "```\n",
        "\n",
        "Here is a grid stride loop in a kernel:\n",
        "\n",
        "```c\n",
        "__global__ void kernel(int *a, int N)\n",
        "{\n",
        "  int indexInGrid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int gridStride = gridDim.x * blockDim.x;\n",
        "\n",
        "  for (int i = indexInGrid; i < N; i += gridStride)\n",
        "  {\n",
        "    // do something to a[i];\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "goaNYWLAPtip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "For this exercise, we use the `%%writefile` magic function to create a file so we can then generate an outputs for a visualization, rather than running it inline with `%%cuda`."
      ],
      "metadata": {
        "id": "tvezV9yK7nNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile grid_stride.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "void init(int *a, int N)\n",
        "{\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i)\n",
        "  {\n",
        "    a[i] = i;\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void squareElements(int *a, int N)\n",
        "{\n",
        "\n",
        "  /*\n",
        "   * Use a grid-stride loop to ensure each thread works\n",
        "   * on more than one array element\n",
        "   */\n",
        "\n",
        "  int idx = // Your code here\n",
        "  int stride = // Your code here\n",
        "\n",
        "  for (int i = idx; i < N; i += stride)\n",
        "  {\n",
        "    int old_value = a[i];\n",
        "    a[i] *= i;\n",
        "    printf(\"Thread %d (Block %d, Thread in Block %d) processing element %d. Old value: %d, New value: %d\\n\",\n",
        "           idx, blockIdx.x, threadIdx.x, i, old_value, a[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "bool checkElementsSquared(int *a, int N)\n",
        "{\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i)\n",
        "  {\n",
        "    if (a[i] != i*i) return false;\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int N = 500;\n",
        "  int *a;\n",
        "\n",
        "  size_t size = N * sizeof(int);\n",
        "\n",
        "  // Use cudaMallocManaged to allocate memory for the array\n",
        "  // and the size variable\n",
        "  // Your code her\n",
        "\n",
        "  init(a, N);\n",
        "\n",
        "  // The size of this grid is 356 (32 x 8)\n",
        "  size_t threads_per_block = 32;\n",
        "  size_t number_of_blocks = 8;\n",
        "\n",
        "  squareElements<<<number_of_blocks, threads_per_block>>>(a, N);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  bool areSquared = checkElementsSquared(a, N);\n",
        "  printf(\"All elements were doubled? %s\\n\", areSquared ? \"TRUE\" : \"FALSE\");\n",
        "\n",
        "  cudaFree(a);\n",
        "}\n"
      ],
      "metadata": {
        "id": "8mukmHl47mvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution\n",
        "\n"
      ],
      "metadata": {
        "id": "A2YiWezgwfMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile grid_stride.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "void init(int *a, int N)\n",
        "{\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i)\n",
        "  {\n",
        "    a[i] = i;\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void squareElements(int *a, int N)\n",
        "{\n",
        "\n",
        "  /*\n",
        "   * Use a grid-stride loop to ensure each thread works\n",
        "   * on more than one array element\n",
        "   */\n",
        "\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = gridDim.x * blockDim.x;\n",
        "\n",
        "  for (int i = idx; i < N; i += stride)\n",
        "  {\n",
        "    int old_value = a[i];\n",
        "    a[i] *= i;\n",
        "    printf(\"Thread %d (Block %d, Thread in Block %d) processing element %d. Old value: %d, New value: %d\\n\",\n",
        "           idx, blockIdx.x, threadIdx.x, i, old_value, a[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "bool checkElementsSquared(int *a, int N)\n",
        "{\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i)\n",
        "  {\n",
        "    if (a[i] != i*i) return false;\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int N = 500;\n",
        "  int *a;\n",
        "\n",
        "  size_t size = N * sizeof(int);\n",
        "  cudaMallocManaged(&a, size);\n",
        "\n",
        "  init(a, N);\n",
        "\n",
        "  // The size of this grid is 356 (32 x 8)\n",
        "  size_t threads_per_block = 32;\n",
        "  size_t number_of_blocks = 8;\n",
        "\n",
        "  squareElements<<<number_of_blocks, threads_per_block>>>(a, N);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  bool areSquared = checkElementsSquared(a, N);\n",
        "  printf(\"All elements were doubled? %s\\n\", areSquared ? \"TRUE\" : \"FALSE\");\n",
        "\n",
        "  cudaFree(a);\n",
        "}\n"
      ],
      "metadata": {
        "id": "rGp9on3NOFYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o grid_stride grid_stride.cu\n",
        "!./grid_stride > output.txt"
      ],
      "metadata": {
        "id": "KaV4V28519rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the grid stride"
      ],
      "metadata": {
        "id": "3132K_ho8mAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from google.colab import files\n",
        "%matplotlib inline\n",
        "\n",
        "def parse_line(line):\n",
        "    pattern = r\"Thread (\\d+) \\(Block (\\d+), Thread in Block (\\d+)\\) processing element (\\d+)\\. Old value: (\\d+), New value: (\\d+)\"\n",
        "    match = re.search(pattern, line)\n",
        "    if match:\n",
        "        return {\n",
        "            'Thread ID': int(match.group(1)),\n",
        "            'Block ID': int(match.group(2)),\n",
        "            'Thread in Block': int(match.group(3)),\n",
        "            'Element Index': int(match.group(4)),\n",
        "            'Old Value': int(match.group(5)),\n",
        "            'New Value': int(match.group(6))\n",
        "        }\n",
        "    return None\n",
        "\n",
        "# Read the output file and parse it into a DataFrame\n",
        "data = []\n",
        "print(\"Reading file...\")\n",
        "with open('output.txt', 'r') as f:\n",
        "    content = f.read()\n",
        "    print(f\"File content (first 500 characters):\\n{content[:500]}\")\n",
        "\n",
        "    # Use regex to find all matches in the entire content\n",
        "    pattern = r\"Thread (\\d+) \\(Block (\\d+), Thread in Block (\\d+)\\) processing element (\\d+)\\. Old value: (\\d+), New value: (\\d+)\"\n",
        "    matches = re.finditer(pattern, content)\n",
        "\n",
        "    for i, match in enumerate(matches, 1):\n",
        "        data.append({\n",
        "            'Thread ID': int(match.group(1)),\n",
        "            'Block ID': int(match.group(2)),\n",
        "            'Thread in Block': int(match.group(3)),\n",
        "            'Element Index': int(match.group(4)),\n",
        "            'Old Value': int(match.group(5)),\n",
        "            'New Value': int(match.group(6))\n",
        "        })\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Processed {i} matches...\")\n",
        "\n",
        "print(f\"Number of parsed data points: {len(data)}\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print DataFrame info for debugging\n",
        "print(\"\\nDataFrame Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nFirst few rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "if df.empty:\n",
        "    print(\"The DataFrame is empty. No visualizations will be created.\")\n",
        "else:\n",
        "    # Create a scatter plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.scatterplot(data=df, x='Element Index', y='Thread ID', hue='Block ID', palette='viridis', s=50)\n",
        "    plt.title('Grid Stride Pattern Visualization')\n",
        "    plt.xlabel('Array Element Index')\n",
        "    plt.ylabel('Thread ID')\n",
        "    plt.legend(title='Block ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Create a heatmap to show the distribution of work across threads and blocks\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    heatmap_data = df.pivot_table(values='Element Index', index='Block ID', columns='Thread in Block', aggfunc='count')\n",
        "    sns.heatmap(heatmap_data, cmap='YlOrRd', annot=True, fmt='d', cbar_kws={'label': 'Number of Elements Processed'})\n",
        "    plt.title('Distribution of Work Across Threads and Blocks')\n",
        "    plt.xlabel('Thread in Block')\n",
        "    plt.ylabel('Block ID')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and print the stride\n",
        "    stride = df.groupby('Thread ID')['Element Index'].diff().dropna().mode().iloc[0]\n",
        "    print(f\"\\nStride (most common difference between consecutive elements for a thread): {stride}\")\n",
        "\n",
        "print(\"Script execution completed.\")"
      ],
      "metadata": {
        "id": "_-3h0kML7hj_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}